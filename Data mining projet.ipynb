{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey ++\n",
      "yoooo\n",
      "hey!!!\n",
      "defaultdict(<class 'int'>, {'GAMBLING': 14218, 'OTHER NARCOTIC VIOLATION': 118, 'STALKING': 3175, 'NON-CRIMINAL (SUBJECT SPECIFIED)': 6, 'WEAPONS VIOLATION': 65364, 'KIDNAPPING': 6489, 'ROBBERY': 245492, 'DECEPTIVE PRACTICE': 241825, 'CRIM SEXUAL ASSAULT': 25298, 'CONCEALED CARRY LICENSE VIOLATION': 143, 'NARCOTICS': 697936, 'ARSON': 10784, 'PUBLIC PEACE VIOLATION': 46470, 'BATTERY': 1182492, 'ASSAULT': 398354, 'DOMESTIC VIOLENCE': 1, 'LIQUOR LAW VIOLATION': 13801, 'NON-CRIMINAL': 124, 'CRIMINAL TRESPASS': 186488, 'BURGLARY': 376088, 'HUMAN TRAFFICKING': 36, 'OBSCENITY': 483, 'THEFT': 1352794, 'OTHER OFFENSE': 401356, 'SEX OFFENSE': 23839, 'PUBLIC INDECENCY': 148, 'MOTOR VEHICLE THEFT': 304008, 'OFFENSE INVOLVING CHILDREN': 42787, 'NON - CRIMINAL': 38, 'RITUALISM': 23, 'CRIMINAL DAMAGE': 743419, 'Primary Type': 1, 'INTIMIDATION': 3766, 'INTERFERENCE WITH PUBLIC OFFICER': 13866, 'PROSTITUTION': 67601, 'HOMICIDE': 8837})\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext #as sc\n",
    "\n",
    "class ExploreData:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \"\"\"\n",
    "    @ This function takes as an argument a source file directory, then calculates for each value of object_id:\n",
    "        the number of sources\n",
    "        the biggest source_id\n",
    "        the minimum and maximum values of ra and decl\n",
    "    \"\"\"\n",
    "    def countData(self, directory, sc):\n",
    "        rdd = sc.textFile(str(directory))\n",
    "        myRdd = rdd.filter(lambda line: len(line) > 0) \\\n",
    "            .map(lambda line: line.split(',')) \\\n",
    "            .filter(lambda line: line[3] != \"NULL\") \\\n",
    "            .map(lambda line: (1,[1]))\n",
    "        return myRdd.count()\n",
    "    \n",
    "    def countCrimesPerType(self, directory, sc):\n",
    "        rdd = sc.textFile(str(directory))\n",
    "        myRdd = rdd.filter(lambda line: len(line) > 0) \\\n",
    "            .map(lambda line: line.split(',')) \\\n",
    "            .filter(lambda line: line[5] != \"NULL\") \\\n",
    "            .map(lambda line: (line[5],[1]))\\\n",
    "            .countByKey()\n",
    "        return myRdd\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "sc = SparkContext.getOrCreate()\n",
    "file  = \"/Users/HAMIZI16000/Documents/programmation/data_mining/Crimes_-_2001_to_present.csv\"\n",
    "toto = ExploreData()\n",
    "#res = toto.countData(file,sc)\n",
    "res = toto.countCrimesPerType(file, sc)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
